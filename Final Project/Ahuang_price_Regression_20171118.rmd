---
title: "Data605_Project3"
author: "Angus Huang"
date: "December 17, 2017"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

```{r}
options(warn = 0)
library(dplyr)
library(MASS)
library(ggplot2)
```


1.Download the dataset from the source.
The original datasets are available from Kaggle.com 
https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
```{r}
mydata <- read.table ("https://github.com/angus001/Data605/raw/master/train.csv", header = T, sep =",")
testdata <- read.table ("https://github.com/angus001/Data605/raw/master/test.csv",header =T, sep =",")

#subset quantitative & a few other variables
mydata2 <-mydata[,which(names(mydata)%in% c("LotFrontage","LotArea","OverallQual","MasVnrArea", "BsmtFinSF1", "BsmtFinSF2","X1stFlrSF", "TotRmsAbvGrd", "GarageCars", "SalePrice","Neighborhood"))]
```



##2 Clean up data 
```{r}

#check how many na in each variable/column
na_count <-sapply(mydata, function(x) sum(length(which(is.na(x)))))
na_count <- data.frame(na_count)
na_count$col_names <- rownames(na_count)

df1 <-filter(na_count, na_count > 0) #filter the dataframe into one that shows columns with na values
head( df1[order(-df1$na_count),]) #sort descending and show column with most na
```


2(a). Produce pair charts to see if any variable might have better relationship with the output-"SalePrice".
"X1stFlrSF" seems to be a great candidate.

```{r}
pairs(mydata2,gap=0.5, col = 'skyblue')
```


```{r}
#assigned zero to na values
mydata2[is.na(mydata2)]<-0

```


Picking a quantitative value and caculate the probabilty P(X>x&Y>y)
```{r}
X <- mydata2$X1stFlrSF

Y <- mydata2$SalePrice

x <-quantile(X, 0.75)
y<-quantile(Y,0.5)
x 
y
```

```{r}
dfpb <- data.frame(X,Y)
dfpb$'P(X>x&Y>y)' <-ifelse (dfpb$X > x & dfpb$Y>y, 1.00,0)

dfpb$'Y>y' <-ifelse(dfpb$Y > y, 1.00,0)

#P(X,Y)/P(Y)
round((sum(dfpb$'P(X>x&Y>y)')/nrow(dfpb))/(sum(dfpb$'Y>y')/nrow(dfpb)), digits = 4)
```


```{r}
sum(dfpb$'P(X>x&Y>y)')/nrow(dfpb)
```

```{r}
dfpb$'P(X<x & Y>y)' <- ifelse( X<x &Y> y, 1,0)
(sum(dfpb$'P(X<x & Y>y)')/nrow(dfpb))/(sum(dfpb$'Y>y')/nrow(dfpb))

```


##Building a regression model
###1. Perform simple linear regression
The R-Squared of 0.3671 indicates the 1st floor square feet ('X1stFlrSF) alone explain about 36% of the variance in saleprice across the selected neighborhoods. The P value is very small and less than 0.05, therefore the model is valid. F-statistic is used for additional check on the validity of R-Sqaured value. R-Squared value explains the strength of the relationship between the (input vs. output) variables.  F-statistic then check if the R-sqaured value is valid or not. Low F-value means close similarity between groups while the high F-value means the opposite.


```{r}
housepricelm <- lm(SalePrice ~ X1stFlrSF,  data = mydata2 )
summary(housepricelm)
```
Plotting a scatter plot with regression line.

```{r}
{plot(mydata2$X1stFlrSF,mydata2$SalePrice,main = "First Floor Square Feet vs. House Prices",
      col = 'skyblue', pch = 16, xlab = "Square Feet", ylab = "Sale Prices")
abline(housepricelm, col = "red")}

```


With Multivariate regression, the 76% of the variance can be explained by the variables. The variables "LotFrontage" and "BsmtFinSF2" have has large P values (>0.05), and were removed in subsequent backward elimination. 
   


```{r}
#Subset data and select variables from pair plot.
housepricelm3 <-lm(SalePrice ~ X1stFlrSF+LotFrontage+LotArea+OverallQual+MasVnrArea+   BsmtFinSF1+BsmtFinSF2+X1stFlrSF+TotRmsAbvGrd+GarageCars,  data = mydata2 )
summary(housepricelm3)
```


Remove variables (LotFrontage & BsmtFinSF2) with large p value (p value > 0.005)
```{r}
housepricelm4 <- update(housepricelm3, .~. -LotFrontage, data = mydata2)
housepricelm4 <- update(housepricelm3, .~. -BsmtFinSF2, data = mydata2)
summary(housepricelm4)
```


Residual analysis shows the plot is slightlty curvilinear. 
```{r}
plot(fitted(housepricelm4),resid(housepricelm4))
```

```{r}
qqnorm(resid(housepricelm4), col = "blue")
qqline(resid(housepricelm4), col = "red")

```

##3 Predict the price with the model build above

3(a) Subset the data into a smaller data frame with variables used in the above model
```{r}
testdata2 <-testdata[,which(names(testdata)%in% c("LotFrontage","LotArea","OverallQual","MasVnrArea", "BsmtFinSF1","Id", "BsmtFinSF2","X1stFlrSF", "TotRmsAbvGrd", "GarageCars","Neighborhood"))]

#check the number of na in data column
na_count2 <-sapply(testdata2, function(x) sum(length(which(is.na(x)))))
na_count2 <- data.frame(na_count2)
na_count2$col_names <- rownames(na_count2)
na_count2 <-filter(na_count2, na_count2 > 0) #filter the dataframe into one that shows columns with na values
na_count2[order(-na_count2$na_count2),] #sort descending and show column with most na

# Assign zero for na values
testdata2[is.na(testdata2)]<-0
```


```{r}
#predict the "SalePrice"
results <-predict(housepricelm4,testdata2)
testdata2$SalePrice <-c(abs(results))
testdata3<-data.frame(testdata2[,c("Id","SalePrice")])

head(testdata3)
#Below line is writing the result to csv file for submission.
#write.csv(testdata3, file= "Myresult.csv", sep=",")
```

```{r }
X<-mydata$X1stFlrSF
Y<-mydata$SalePrice
df<-data.frame(X,Y)
```

```{r}
ggplot(df, aes(x=X)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="skyblue")+
  geom_density(alpha=0.5) +
  xlab("1st Floor Square Feet") +
  geom_vline(aes(xintercept=median(df$X)),
             color="green", linetype="dashed", size=1)

```

```{r}
ggplot(df, aes(x=Y)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="skyblue")+
  geom_density(alpha=0.5) +
  xlab("House Sale Prices") +
  geom_vline(aes(xintercept=median(df$Y)),
             color="green", linetype="dashed", size=1)

```


Perform boxcox analysis to find log-Likelihood. Look for Lambda value with max likelihood. THe max likelihood is at 0.06 power.
```{r}
bc = boxcox(Y~X, data = df)
```
```{r}
lamda =bc$x
likelihood = bc$y
bc1=cbind(lamda,likelihood)
head(bc1[order(-likelihood),])
```

```{r}
df$Ypower = (df$Y)^3/50
powerreg <- lm(Ypower~X, df)
hist(powerreg$residuals)
```
```{r}
qqnorm(powerreg$residuals)
qqline(powerreg$residuals)
```

Perform a correlation test between variables. The correlation test shows a correlation of 0.605 without tranforming the variable. The correlation actually become less to 0.441 after transforming the variable.
```{r}
cor(df)

```
```{r}
cor.test(df$X,df$Y, conf.level = 0.99)
```

Fitting the data point into different distribution to understand the underlying spread of the data.
```{r}
fit <- fitdistr(df$X, densfun = 'cauchy')
fit
```

```{r}
lamda2 <- fit$estimate
lamda2
```
Take 1000 samples from the distribution, plot a histogram and compare with the non-transformed original values.
```{r}
estimate <- rexp(1000,lamda2)
hist(estimate, breaks = 200)
```
```{r}
hist(df$X, breaks = 200)
```

![Kaggle Result](https://github.com/angus001/Data605/blob/master/kagglefirsttry.PNG?raw=true)

```{r}
#![Kaggle Result](https://github.com/angus001/Data605/blob/master/kagglefirsttry.PNG?raw=true)
```